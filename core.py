# core.py
import random
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama.chat_models import ChatOllama
from langchain_core.prompts import PromptTemplate

# ==========================================
# ğŸ› ï¸ AYARLAR
# ==========================================
TEST_K = 3
TEST_MMR = True
DB_DIR = "db"


# ==========================================

def get_mock_weather(city="ElazÄ±ÄŸ"):
    """
    GerÃ§ek bir API yerine rastgele hava durumu dÃ¶ndÃ¼ren MOCK (Taklit) fonksiyon.
    """
    if not city:
        city = "ElazÄ±ÄŸ"

    conditions = [
        "GÃ¼neÅŸli â˜€ï¸, 25Â°C",
        "SaÄŸanak YaÄŸÄ±ÅŸlÄ± ğŸŒ§ï¸, 10Â°C",
        "KarlÄ± â„ï¸, -2Â°C",
        "RÃ¼zgarlÄ± ğŸ’¨, 18Â°C"
    ]
    forecast = random.choice(conditions)
    return f"{city} ÅŸehri iÃ§in hava durumu ÅŸu an: {forecast}."


def get_retriever():
    emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vect_db = Chroma(persist_directory=DB_DIR, embedding_function=emb)
    return vect_db.as_retriever(search_kwargs={"k": TEST_K})


def get_llm():
    return ChatOllama(model="llama3", temperature=0.1)


# !!! GÃœNCELLENMÄ°Å PROMPT !!!
# Hem belgeyi (Document) hem de API bilgisini (External Info) iÃ§eriyor.
# core.py iÃ§indeki PROMPT_TEMPLATE kÄ±smÄ±nÄ± tamamen bununla deÄŸiÅŸtirin:

PROMPT_TEMPLATE = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
Sen SADECE TÃœRKÃ‡E konuÅŸan yardÄ±mcÄ± bir asistansÄ±n. Asla Ä°ngilizce cevap verme. AÅŸaÄŸÄ±daki "BaÄŸlam" ve "DÄ±ÅŸ Bilgi" kÄ±sÄ±mlarÄ±nÄ± kullanarak soruya cevap ver.

Kurallar:
1. CEVABI KESÄ°NLÄ°KLE TÃœRKÃ‡E OLARAK VER.
2. Ã–ncelikle sorunun cevabÄ±nÄ± belgelerdeki (BaÄŸlam) kurallara gÃ¶re ver.
3. ArdÄ±ndan, verilen "DÄ±ÅŸ Bilgi"ye (Hava Durumu) dayanarak kullanÄ±cÄ±ya kÄ±sa bir tavsiye ekle.
4. CevabÄ± uydurma.<|eot_id|><|start_header_id|>user<|end_header_id|>

DÄ±ÅŸ Bilgi (API):
{api_context}

BaÄŸlam (Belgeler):
{context}

Soru: {question} (CevabÄ± TÃ¼rkÃ§e ver)<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""


def answer_question(question: str, retriever, city_name="ElazÄ±ÄŸ"):
    """
    Parametreler:
    - city_name: ArayÃ¼zden gelen ÅŸehir bilgisi (Hava durumu iÃ§in)
    """

    # 1. API Verisini Ã‡ek (Mock)
    api_data = get_mock_weather(city_name)
    print(f"ğŸŒ API Ã‡aÄŸrÄ±sÄ± YapÄ±ldÄ±: {api_data}")

    # 2. Belgeleri Getir (RAG)
    if TEST_MMR:
        retriever.search_type = "mmr"
        retriever.search_kwargs = {"k": TEST_K, "fetch_k": TEST_K * 4}
    else:
        retriever.search_type = "similarity"
        retriever.search_kwargs = {"k": TEST_K}

    docs = retriever.invoke(question)

    # 3. Prompt'u HazÄ±rla (Belge + API)
    context_text = "\n\n".join(d.page_content for d in docs)[:7000]

    # Prompt'a hem context'i hem api_context'i gÃ¶nderiyoruz
    final_prompt = PROMPT_TEMPLATE.format(
        question=question,
        context=context_text,
        api_context=api_data
    )

    llm = get_llm()
    result = llm.invoke(final_prompt)

    clean_result = result.content.strip()

    # KaynaklarÄ± Listele
    srcs = []
    for d in docs:
        meta = d.metadata or {}
        s = meta.get("source", "?")
        if "page" in meta:
            s += f" (sayfa {meta['page'] + 1})"
        srcs.append(s)
    uniq_srcs = list(dict.fromkeys(srcs))

    # CevabÄ±n altÄ±na API bilgisini de ekleyelim ki kullanÄ±cÄ± neye gÃ¶re tavsiye verdiÄŸimizi gÃ¶rsÃ¼n
    clean_result += f"\n\nğŸŒ¤ï¸ (Referans alÄ±nan hava durumu: {api_data})"

    return clean_result, uniq_srcs