# index_build.py
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
# DEÄžÄ°ÅžEN Ä°MPORTLAR:
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

DATA_DIR = Path("data")
DB_DIR = "db"


def load_docs(data_dir: Path):
    """Verilen klasÃ¶rdeki PDF, TXT ve MD dosyalarÄ±nÄ± yÃ¼kler."""
    docs = []
    print("ðŸ”Ž Desteklenen dosyalar aranÄ±yor...")
    for p in data_dir.glob("**/*"):
        p_str = str(p)
        if p.suffix.lower() == ".pdf":
            docs.extend(PyPDFLoader(p_str).load())
        elif p.suffix.lower() in {".txt", ".md"}:
            docs.extend(TextLoader(p_str, encoding="utf-8").load())
    return docs


def build_index():
    print("ðŸ”Ž Belgeler yÃ¼kleniyor...")
    raw_docs = load_docs(DATA_DIR)

    if not raw_docs:
        print(f"âš ï¸ {DATA_DIR} klasÃ¶rÃ¼nde desteklenen (PDF, TXT, MD) belge bulunamadÄ±. Index oluÅŸturulmadÄ±.")
        return

    print(f"âž¡ï¸ {len(raw_docs)} dokÃ¼man/sayfa bulundu.")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=150, separators=["\n\n", "\n", ".", " "]
    )

    chunks = splitter.split_documents(raw_docs)
    print(f"ðŸ§© {len(chunks)} parÃ§a (chunk) oluÅŸturuldu.")

    # DEÄžÄ°ÅžEN SINIF ADI: HuggingFaceBgeEmbeddings -> HuggingFaceEmbeddings
    emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # Chroma VeritabanÄ± OluÅŸturma
    vectordb = Chroma.from_documents(chunks, emb, persist_directory=DB_DIR)

    vectordb.persist()

    print(f"âœ… Chroma index oluÅŸturuldu: {DB_DIR}/")


if __name__ == "__main__":
    Path("data").mkdir(exist_ok=True)
    build_index()